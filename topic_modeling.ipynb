{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import pyLDAvis.gensim \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Inspect Data for Missing or Duplicated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"data/stocks_raw_454_posts.csv\")\n",
    "\n",
    "# # check for missing values\n",
    "# print(\"missing values per column:\\\\n\", df.isnull().sum())\n",
    "# # drop rows where 'Text' is empty\n",
    "# df = df[df[\"Text\"].str.strip() != \"\"]\n",
    "# # check for duplicates (based on title + text)\n",
    "# df.drop_duplicates(subset=[\"Title\", \"Text\"], inplace=True)\n",
    "\n",
    "# # save cleaned data\n",
    "# df.to_csv(\"data/stocks_cleaned.csv\", index=False)\n",
    "# print(f\"cleaned dataset saved: {len(df)} posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine The Datasets Into a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [\"data/stocks_raw_879_posts.csv\"]\n",
    "df_list = [pd.read_csv(file) for file in csv_files]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "df.drop_duplicates(subset=[\"Title\", \"Text\"], inplace=True)\n",
    "\n",
    "df.to_csv(\"data/stocks_combined.csv\", index=False)\n",
    "print(f\"combined dataset saved: {len(df)} posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df = pd.read_csv(\"data/stocks_combined.csv\")\n",
    "\n",
    "# preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):  # handle missing values\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\\\S+', '', text)\n",
    "    text = re.sub(r'[^a-z\\\\s]', '', text)\n",
    "    tokens = word_tokenize(text)  # tokenize\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')] \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# apply preprocessing to the \"Text\" column\n",
    "df[\"Cleaned_Text\"] = df[\"Text\"].astype(str).apply(preprocess_text)\n",
    "\n",
    "# save the cleaned dataset\n",
    "df.to_csv(\"data/stocks_preprocessed.csv\", index=False)\n",
    "print(f\"preprocessing complete! cleaned dataset saved: {len(df)} posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# tokenize the cleaned text\n",
    "df[\"Tokenized\"] = df[\"Cleaned_Text\"].apply(lambda x: x.split())\n",
    "\n",
    "# create a dict and corpus\n",
    "dictionary = corpora.Dictionary(df[\"Tokenized\"])\n",
    "corpus = [dictionary.doc2bow(text) for text in df[\"Tokenized\"]]\n",
    "\n",
    "# training‚ÄºÔ∏è (adjust num_topics for different results)\n",
    "num_topics = 10  # number of topics\n",
    "lda_model = LdaModel(corpus=corpus,\n",
    "                     id2word=dictionary,\n",
    "                     num_topics=num_topics,\n",
    "                     alpha='auto',\n",
    "                     eta='auto',\n",
    "                     passes=10) # ‚ú®the model‚ú®\n",
    "\n",
    "# print the topics\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"üí° Topic {idx}: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize w/ pyLDAvis\n",
    "# pyLDAvis.enable_notebook()\n",
    "# vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "# pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get The Topic Distribution For Each Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_distribution(lda_model, corpus):\n",
    "    topic_dist = []\n",
    "    for doc in corpus:\n",
    "        topic_probs = lda_model.get_document_topics(doc, minimum_probability=0)\n",
    "        topic_dist.append([prob for _, prob in topic_probs])\n",
    "    \n",
    "    return pd.DataFrame(topic_dist, columns=[f\"Topic {i}\" for i in range(lda_model.num_topics)])\n",
    "\n",
    "# dataframe\n",
    "topic_df = get_topic_distribution(lda_model, corpus)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(topic_df, cmap=\"coolwarm\", annot=False, cbar=True)\n",
    "plt.title(\"Topic Distribution Across Documents\")\n",
    "plt.xlabel(\"Topics\")\n",
    "plt.ylabel(\"Documents\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distribution of Word Counts in Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.colors as mcolors\n",
    "\n",
    "# cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(16, 14), dpi=160, sharex=True, sharey=True)\n",
    "\n",
    "# for i, ax in enumerate(axes.flatten()):\n",
    "#     df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.Dominant_Topic == i, :]\n",
    "    \n",
    "#     # Check that 'Text' is a string and then calculate document lengths\n",
    "#     doc_lens = [len(d.split()) if isinstance(d, str) else 0 for d in df_dominant_topic_sub.Text]\n",
    "    \n",
    "#     ax.hist(doc_lens, bins=1000, color=cols[i])\n",
    "#     ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n",
    "    \n",
    "#     # Seaborn warning fix: update `shade=False` to `fill=False`\n",
    "#     sns.kdeplot(doc_lens, color=\"black\", fill=False, ax=ax.twinx())\n",
    "    \n",
    "#     ax.set(xlim=(0, 1000), xlabel='Document Word Count')\n",
    "#     ax.set_ylabel('Number of Documents', color=cols[i])\n",
    "#     ax.set_title('Topic: ' + str(i), fontdict=dict(size=16, color=cols[i]))\n",
    "\n",
    "# fig.tight_layout()\n",
    "# fig.subplots_adjust(top=0.90)\n",
    "# plt.xticks(np.linspace(0, 1000, 9))\n",
    "# fig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=22)\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
