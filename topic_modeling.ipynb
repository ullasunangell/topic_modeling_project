{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import pyLDAvis.gensim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Data for Missing or Duplicated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/stocks_raw_454_posts.csv\")\n",
    "\n",
    "# check for missing values\n",
    "print(\"Missing values per column:\\\\n\", df.isnull().sum())\n",
    "# drop rows where 'Text' is empty\n",
    "df = df[df[\"Text\"].str.strip() != \"\"]\n",
    "# check for duplicates (based on title + text)\n",
    "df.drop_duplicates(subset=[\"Title\", \"Text\"], inplace=True)\n",
    "\n",
    "# save cleaned data\n",
    "df.to_csv(\"data/stocks_cleaned.csv\", index=False)\n",
    "print(f\"‚úÖ Cleaned dataset saved: {len(df)} posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine The Datasets Into a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [\"data/stocks_raw_500_posts.csv\", \"data/stocks_raw_454_posts.csv\"]\n",
    "df_list = [pd.read_csv(file) for file in csv_files]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "df.drop_duplicates(subset=[\"Title\", \"Text\"], inplace=True)\n",
    "\n",
    "df.to_csv(\"data/stocks_combined.csv\", index=False)\n",
    "print(f\"combined dataset saved: {len(df)} posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df = pd.read_csv(\"data/stocks_combined.csv\")\n",
    "\n",
    "# preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):  # handle missing values\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\\\S+', '', text)\n",
    "    text = re.sub(r'[^a-z\\\\s]', '', text)\n",
    "    tokens = word_tokenize(text)  # tokenize\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')] \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# apply preprocessing to the \"Text\" column\n",
    "df[\"Cleaned_Text\"] = df[\"Text\"].astype(str).apply(preprocess_text)\n",
    "\n",
    "# save the cleaned dataset\n",
    "df.to_csv(\"data/stocks_preprocessed.csv\", index=False)\n",
    "print(f\"preprocessing complete! cleaned dataset saved: {len(df)} posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# tokenize the cleaned text\n",
    "df[\"Tokenized\"] = df[\"Cleaned_Text\"].apply(lambda x: x.split())\n",
    "\n",
    "# create a dict and corpus\n",
    "dictionary = corpora.Dictionary(df[\"Tokenized\"])\n",
    "corpus = [dictionary.doc2bow(text) for text in df[\"Tokenized\"]]\n",
    "\n",
    "# training‚ÄºÔ∏è (adjust num_topics for different results)\n",
    "num_topics = 5  # number of topics\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=10) #‚ú®the model‚ú®\n",
    "\n",
    "# print the topics\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"üí° Topic {idx}: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize w/ pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the topic distribution for each document\n",
    "def get_topic_distribution(lda_model, corpus):\n",
    "    topic_dist = []\n",
    "    for doc in corpus:\n",
    "        topic_probs = lda_model.get_document_topics(doc, minimum_probability=0)\n",
    "        topic_dist.append([prob for _, prob in topic_probs])\n",
    "    \n",
    "    return pd.DataFrame(topic_dist, columns=[f\"Topic {i}\" for i in range(lda_model.num_topics)])\n",
    "\n",
    "# dataframe\n",
    "topic_df = get_topic_distribution(lda_model, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(topic_df, cmap=\"coolwarm\", annot=False, cbar=True)\n",
    "plt.title(\"Topic Distribution Across Documents\")\n",
    "plt.xlabel(\"Topics\")\n",
    "plt.ylabel(\"Documents\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
