{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import pyLDAvis.gensim \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prevent file path leaks\n",
    "# import multiprocessing\n",
    "# multiprocessing.set_start_method('forkserver', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Inspect Data for Missing or Duplicated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/multiple_subreddits_4812_posts.csv\")\n",
    "\n",
    "# check for missing values\n",
    "print(\"missing values per column:\\\\n\", df.isnull().sum())\n",
    "# drop rows where 'Text' is empty\n",
    "df = df[df[\"Text\"].str.strip() != \"\"]\n",
    "# check for duplicates (based on title + text)\n",
    "df.drop_duplicates(subset=[\"Title\", \"Text\"], inplace=True)\n",
    "\n",
    "# save cleaned data\n",
    "df.to_csv(\"data/subreddits_cleaned.csv\", index=False)\n",
    "print(f\"cleaned dataset saved: {len(df)} posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine The Datasets Into a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_files = [\"data/stocks_raw_879_posts.csv\"]\n",
    "# df_list = [pd.read_csv(file) for file in csv_files]\n",
    "# df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# df.drop_duplicates(subset=[\"Title\", \"Text\"], inplace=True)\n",
    "\n",
    "# df.to_csv(\"data/stocks_combined.csv\", index=False)\n",
    "# print(f\"combined dataset saved: {len(df)} posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "df = pd.read_csv(\"data/subreddits_cleaned.csv\")\n",
    "\n",
    "# preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):  # handle missing values\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\\\S+', '', text)\n",
    "    text = re.sub(r'[^a-z\\\\s]', '', text)\n",
    "    tokens = word_tokenize(text)  # tokenize\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')] \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# apply preprocessing to the \"Text\" column\n",
    "df[\"Cleaned_Text\"] = df[\"Text\"].astype(str).apply(preprocess_text)\n",
    "\n",
    "# save the cleaned dataset\n",
    "df.to_csv(\"data/stocks_preprocessed.csv\", index=False)\n",
    "print(f\"preprocessing complete! cleaned dataset saved: {len(df)} posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "df = pd.read_csv(\"data/stocks_preprocessed.csv\")\n",
    "# Ensure 'Cleaned_Text' is treated as a string and handle NaN values\n",
    "df[\"Cleaned_Text\"] = df[\"Cleaned_Text\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Now tokenize safely\n",
    "df[\"Tokenized\"] = df[\"Cleaned_Text\"].apply(lambda x: x.split())\n",
    "\n",
    "# Create dictionary & corpus\n",
    "dictionary = corpora.Dictionary(df[\"Tokenized\"])\n",
    "corpus = [dictionary.doc2bow(text) for text in df[\"Tokenized\"]]\n",
    "\n",
    "# training‚ÄºÔ∏è (adjust num_topics for different results)\n",
    "num_topics = 5  # number of topics\n",
    "lda_model = LdaModel(corpus=corpus,\n",
    "                     id2word=dictionary,\n",
    "                     num_topics=num_topics,\n",
    "                     alpha='auto',\n",
    "                     eta='auto',\n",
    "                     passes=1000) # ‚ú®the model‚ú®\n",
    "\n",
    "# topics with word weights\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"\\nüí° topic {idx}:\\n {topic}\")\n",
    "\n",
    "# topics without weightss\n",
    "print(\"\\nüí°üí°üí°top words in each topic:\")\n",
    "for topic_id, words in lda_model.show_topics(num_topics=num_topics, num_words=10, formatted=False):\n",
    "    word_list = [word[0] for word in words]\n",
    "    print(f\"üí° topic {topic_id}: {', '.join(word_list)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize w/ pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(vis)\n",
    "# pyLDAvis.save_html(vis, 'lda_visualization.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get The Topic Distribution For Each Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get topic probabilities for each document\n",
    "topic_distributions = np.array([[prob for _, prob in topic] for topic in lda_model.get_document_topics(corpus, minimum_probability=0)])\n",
    "\n",
    "# Convert to DataFrame\n",
    "topic_df = pd.DataFrame(topic_distributions, columns=[f\"Topic {i}\" for i in range(topic_distributions.shape[1])])\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(topic_df, cmap=\"coolwarm\", annot=False)\n",
    "plt.title(\"Topic Distribution Heatmap\")\n",
    "plt.xlabel(\"Topics\")\n",
    "plt.ylabel(\"Documents\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=df[\"Tokenized\"], dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Documents Per Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "topic_counts = [sum(1 for doc in lda_model[corpus] if max(doc, key=lambda x: x[1])[0] == i) for i in range(num_topics)]\n",
    "sns.barplot(x=[f\"Topic {i}\" for i in range(num_topics)], y=topic_counts)\n",
    "plt.title(\"Number of Documents per Topic\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency Distribution of Word Counts in Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.colors as mcolors\n",
    "\n",
    "# cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(16, 14), dpi=160, sharex=True, sharey=True)\n",
    "\n",
    "# for i, ax in enumerate(axes.flatten()):\n",
    "#     df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.Dominant_Topic == i, :]\n",
    "    \n",
    "#     # Check that 'Text' is a string and then calculate document lengths\n",
    "#     doc_lens = [len(d.split()) if isinstance(d, str) else 0 for d in df_dominant_topic_sub.Text]\n",
    "    \n",
    "#     ax.hist(doc_lens, bins=1000, color=cols[i])\n",
    "#     ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n",
    "    \n",
    "#     # Seaborn warning fix: update `shade=False` to `fill=False`\n",
    "#     sns.kdeplot(doc_lens, color=\"black\", fill=False, ax=ax.twinx())\n",
    "    \n",
    "#     ax.set(xlim=(0, 1000), xlabel='Document Word Count')\n",
    "#     ax.set_ylabel('Number of Documents', color=cols[i])\n",
    "#     ax.set_title('Topic: ' + str(i), fontdict=dict(size=16, color=cols[i]))\n",
    "\n",
    "# fig.tight_layout()\n",
    "# fig.subplots_adjust(top=0.90)\n",
    "# plt.xticks(np.linspace(0, 1000, 9))\n",
    "# fig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=22)\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
